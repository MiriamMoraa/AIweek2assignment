"""
SDG 3: Good Health and Well-being
Diabetes Risk Classification using Machine Learning
Binary Classification with Logistic Regression

Dataset: Pima Indians Diabetes Database
Source: UCI Machine Learning Repository / Kaggle
"""


# 1. IMPORT LIBRARIES

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report,
                             roc_curve, roc_auc_score)
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

print("="*70)
print("SDG 3: DIABETES RISK PREDICTION MODEL")
print("Binary Classification using Logistic Regression")
print("="*70)


# 2. LOAD DATASET

print("\n[STEP 1] Loading Dataset...")

# Sample dataset creation (In practice, load from CSV)
# Download from: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database
# For this demo, we'll create synthetic data that mimics the real dataset

np.random.seed(42)
n_samples = 768

# Generate synthetic data with realistic distributions
data = {
    'Pregnancies': np.random.poisson(3, n_samples),
    'Glucose': np.random.normal(120, 30, n_samples).clip(0, 200),
    'BloodPressure': np.random.normal(70, 12, n_samples).clip(0, 120),
    'SkinThickness': np.random.normal(20, 15, n_samples).clip(0, 100),
    'Insulin': np.random.gamma(2, 40, n_samples).clip(0, 850),
    'BMI': np.random.normal(32, 7, n_samples).clip(15, 70),
    'DiabetesPedigreeFunction': np.random.gamma(2, 0.2, n_samples).clip(0, 2.5),
    'Age': np.random.gamma(5, 6, n_samples).clip(21, 81).astype(int),
}

# Create outcome variable (diabetes) with realistic correlations
diabetes_risk = (
    0.02 * data['Glucose'] +
    0.03 * data['BMI'] +
    0.01 * data['Age'] +
    0.5 * data['DiabetesPedigreeFunction'] +
    np.random.normal(0, 2, n_samples) - 3
)
data['Outcome'] = (diabetes_risk > np.percentile(diabetes_risk, 65)).astype(int)

df = pd.DataFrame(data)

print(f"✓ Dataset loaded successfully!")
print(f"  Total samples: {len(df)}")
print(f"  Features: {df.shape[1] - 1}")
print(f"  Target variable: Outcome (0=No Diabetes, 1=Diabetes)")


# 3. EXPLORATORY DATA ANALYSIS

print("\n[STEP 2] Exploratory Data Analysis...")

print("\nDataset Overview:")
print(df.head())

print("\nDataset Statistics:")
print(df.describe())

print("\nClass Distribution:")
print(df['Outcome'].value_counts())
print(f"  Diabetic: {df['Outcome'].sum()} ({df['Outcome'].mean()*100:.1f}%)")
print(f"  Non-Diabetic: {(df['Outcome']==0).sum()} ({(1-df['Outcome'].mean())*100:.1f}%)")

print("\nMissing Values:")
print(df.isnull().sum())

# Visualize class distribution
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
df['Outcome'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'])
plt.title('Class Distribution', fontsize=14, fontweight='bold')
plt.xlabel('Outcome')
plt.ylabel('Count')
plt.xticks([0, 1], ['No Diabetes', 'Diabetes'], rotation=0)

plt.subplot(1, 3, 2)
df.boxplot(column='Glucose', by='Outcome')
plt.title('Glucose Levels by Diabetes Status')
plt.suptitle('')
plt.xlabel('Outcome (0=No, 1=Yes)')
plt.ylabel('Glucose (mg/dL)')

plt.subplot(1, 3, 3)
df.boxplot(column='BMI', by='Outcome')
plt.title('BMI by Diabetes Status')
plt.suptitle('')
plt.xlabel('Outcome (0=No, 1=Yes)')
plt.ylabel('BMI')

plt.tight_layout()
plt.savefig('eda_visualization.png', dpi=300, bbox_inches='tight')
print("\n✓ EDA visualization saved as 'eda_visualization.png'")

# Correlation heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            square=True, linewidths=1)
plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
print("✓ Correlation heatmap saved as 'correlation_heatmap.png'")

# 4. DATA PREPROCESSING

print("\n[STEP 3] Data Preprocessing...")

# Separate features and target
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Handle zero values (in real dataset, zeros in some columns indicate missing data)
# For this demo, we'll keep all data
print(f"  Original dataset: {X.shape[0]} samples")

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"  Training set: {X_train.shape[0]} samples")
print(f"  Testing set: {X_test.shape[0]} samples")

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("✓ Features scaled using StandardScaler")
print(f"  Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}")


# 5. MODEL TRAINING

print("\n[STEP 4] Training Logistic Regression Model...")

# Initialize and train the model
model = LogisticRegression(
    random_state=42,
    max_iter=1000,
    C=1.0  # Regularization parameter
)

model.fit(X_train_scaled, y_train)
print("✓ Model trained successfully!")

# Display feature importance (coefficients)
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_[0]
}).sort_values('Coefficient', ascending=False)

print("\nFeature Importance (Coefficients):")
print(feature_importance)

# 6. MODEL EVALUATION

print("\n[STEP 5] Model Evaluation...")

# Make predictions
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)
y_test_proba = model.predict_proba(X_test_scaled)[:, 1]

# Calculate metrics
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred)
recall = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
auc_roc = roc_auc_score(y_test, y_test_proba)

print("\n" + "="*70)
print("MODEL PERFORMANCE METRICS")
print("="*70)
print(f"Training Accuracy:    {train_accuracy*100:.2f}%")
print(f"Testing Accuracy:     {test_accuracy*100:.2f}%")
print(f"Precision:            {precision*100:.2f}%")
print(f"Recall (Sensitivity): {recall*100:.2f}%")
print(f"F1-Score:             {f1*100:.2f}%")
print(f"AUC-ROC Score:        {auc_roc:.4f}")
print("="*70)

# Cross-validation
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"\n5-Fold Cross-Validation Accuracy: {cv_scores.mean()*100:.2f}% (+/- {cv_scores.std()*100:.2f}%)")

# Classification Report
print("\nDetailed Classification Report:")
print(classification_report(y_test, y_test_pred, 
                          target_names=['No Diabetes', 'Diabetes']))

# Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(cm)


# 7. VISUALIZE RESULTS
print("\n[STEP 6] Generating Visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Confusion Matrix Heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
axes[0, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')
axes[0, 0].set_ylabel('Actual')
axes[0, 0].set_xlabel('Predicted')

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)
axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {auc_roc:.2f})')
axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
axes[0, 1].set_xlim([0.0, 1.0])
axes[0, 1].set_ylim([0.0, 1.05])
axes[0, 1].set_xlabel('False Positive Rate')
axes[0, 1].set_ylabel('True Positive Rate')
axes[0, 1].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[0, 1].legend(loc="lower right")
axes[0, 1].grid(True, alpha=0.3)

# Feature Importance
feature_importance_sorted = feature_importance.sort_values('Coefficient')
axes[1, 0].barh(feature_importance_sorted['Feature'], 
                feature_importance_sorted['Coefficient'],
                color=['#e74c3c' if x < 0 else '#2ecc71' for x in feature_importance_sorted['Coefficient']])
axes[1, 0].set_xlabel('Coefficient Value')
axes[1, 0].set_title('Feature Importance', fontsize=14, fontweight='bold')
axes[1, 0].axvline(x=0, color='black', linestyle='-', linewidth=0.8)

# Model Performance Comparison
metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
metrics_values = [test_accuracy*100, precision*100, recall*100, f1*100]
bars = axes[1, 1].bar(metrics_names, metrics_values, 
                      color=['#3498db', '#9b59b6', '#e74c3c', '#f39c12'])
axes[1, 1].set_ylim([0, 100])
axes[1, 1].set_ylabel('Score (%)')
axes[1, 1].set_title('Performance Metrics', fontsize=14, fontweight='bold')
axes[1, 1].grid(axis='y', alpha=0.3)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.savefig('model_results.png', dpi=300, bbox_inches='tight')
print("✓ Results visualization saved as 'model_results.png'")


# 8. PREDICTION EXAMPLE

print("\n[STEP 7] Sample Prediction...")

# Example patient data
sample_patient = pd.DataFrame({
    'Pregnancies': [2],
    'Glucose': [148],
    'BloodPressure': [72],
    'SkinThickness': [35],
    'Insulin': [0],
    'BMI': [33.6],
    'DiabetesPedigreeFunction': [0.627],
    'Age': [50]
})

print("\nSample Patient Data:")
print(sample_patient)

# Scale and predict
sample_scaled = scaler.transform(sample_patient)
prediction = model.predict(sample_scaled)[0]
probability = model.predict_proba(sample_scaled)[0]

print(f"\nPrediction: {'DIABETES RISK DETECTED' if prediction == 1 else 'NO DIABETES RISK'}")
print(f"Confidence: {probability[prediction]*100:.2f}%")
print(f"Probability Breakdown:")
print(f"  No Diabetes: {probability[0]*100:.2f}%")
print(f"  Diabetes:    {probability[1]*100:.2f}%")


# 9. SAVE MODEL

print("\n[STEP 8] Saving Model...")

import pickle

# Save model and scaler
with open('diabetes_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("✓ Model saved as 'diabetes_model.pkl'")
print("✓ Scaler saved as 'scaler.pkl'")


# 10. ETHICAL CONSIDERATIONS

print("\n" + "="*70)
print("ETHICAL CONSIDERATIONS")
print("="*70)
print("""
1. DATA BIAS:
   - Dataset is from Pima Indian population; may not generalize to other ethnicities
   - Model should be validated on diverse populations before deployment
   
2. FAIRNESS:
   - Should not replace professional medical diagnosis
   - Ensures accessibility for underserved communities
   - Transparent decision-making process
   
3. PRIVACY:
   - Patient data must be anonymized and securely stored
   - Compliance with HIPAA/GDPR regulations required
   
4. IMPACT:
   - False negatives (missing diabetes cases) are more critical than false positives
   - Recall optimization prioritized over precision
   - Model should assist, not replace, healthcare professionals
   
5. SUSTAINABILITY:
   - Early detection reduces long-term healthcare costs
   - Preventive care aligns with SDG 3 targets
   - Accessible screening in resource-limited settings
""")
print("="*70)

print("\n✓ PROJECT COMPLETE!")
print("\nGenerated Files:")
print("  1. eda_visualization.png - Exploratory data analysis charts")
print("  2. correlation_heatmap.png - Feature correlation matrix")
print("  3. model_results.png - Model performance visualizations")
print("  4. diabetes_model.pkl - Trained model")
print("  5. scaler.pkl - Feature scaler")

print("\n" + "="*70)
print("SDG 3 CONTRIBUTION:")
print("This model contributes to Target 3.4: Reduce premature mortality")
print("from non-communicable diseases through early detection and prevention.")
print("="*70)
